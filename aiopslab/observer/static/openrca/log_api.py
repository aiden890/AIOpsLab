# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

"""OpenRCA Log Observer API - reads log data from CSV files."""

import logging
from pathlib import Path
from datetime import datetime
from typing import Optional
import pandas as pd

from aiopslab.observer.static.base import StaticObserverBase

logger = logging.getLogger(__name__)


class OpenRCALogAPI(StaticObserverBase):
    """
    Observer API for OpenRCA log telemetry.

    Reads log data from CSV files generated by the static replayer.
    Supports time-windowed queries and keyword search.
    """

    def __init__(self, output_path: Path):
        """
        Initialize OpenRCA Log API.

        Args:
            output_path: Path to telemetry output directory containing log.csv
        """
        super().__init__(output_path)
        self.telemetry_type = 'log'

        # Expected schema for OpenRCA log data
        self.expected_columns = [
            'log_id', 'timestamp', 'cmdb_id',
            'log_name', 'value'
        ]

    def get_csv_path(self) -> Path:
        """Return path to log.csv file."""
        return self.output_path / 'log.csv'

    def extract_data(self, start_time: datetime, end_time: datetime, **filters) -> pd.DataFrame:
        """
        Extract log data within time window with optional filters.

        Args:
            start_time: Start of time window
            end_time: End of time window
            **filters: Optional filters (cmdb_id, keyword, log_name)

        Returns:
            DataFrame with filtered log data
        """
        return self.query_logs(
            start_time=start_time,
            end_time=end_time,
            cmdb_id=filters.get('cmdb_id'),
            keyword=filters.get('keyword'),
            log_name=filters.get('log_name')
        )

    def query_logs(
        self,
        start_time: datetime,
        end_time: datetime,
        cmdb_id: Optional[str] = None,
        keyword: Optional[str] = None,
        log_name: Optional[str] = None
    ) -> pd.DataFrame:
        """
        Query log data within a time window with optional filters.

        Args:
            start_time: Start of time window
            end_time: End of time window
            cmdb_id: Optional filter for specific service/component
            keyword: Optional keyword to search in log value (case-insensitive)
            log_name: Optional filter for specific log name

        Returns:
            DataFrame with log entries matching the query

        Raises:
            ValueError: If start_time > end_time
        """
        # Validate time range
        if start_time > end_time:
            raise ValueError(f"start_time ({start_time}) must be <= end_time ({end_time})")

        logger.info(f"Querying logs: {start_time} to {end_time}")
        if cmdb_id:
            logger.info(f"  Filter: cmdb_id={cmdb_id}")
        if keyword:
            logger.info(f"  Filter: keyword='{keyword}'")
        if log_name:
            logger.info(f"  Filter: log_name={log_name}")

        # Read CSV with time filtering
        df = self._read_csv_with_time_filter(start_time, end_time)

        if df.empty:
            logger.info("No logs found in time window")
            return df

        # Apply additional filters
        if cmdb_id:
            df = df[df['cmdb_id'] == cmdb_id]

        if log_name:
            df = df[df['log_name'] == log_name]

        if keyword:
            # Case-insensitive keyword search in log value
            if 'value' in df.columns:
                df = df[df['value'].str.contains(keyword, case=False, na=False)]

        logger.info(f"Found {len(df)} log entries")
        return df

    def search_logs(
        self,
        keyword: str,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
        cmdb_id: Optional[str] = None
    ) -> pd.DataFrame:
        """
        Search logs by keyword with optional time and service filters.

        Args:
            keyword: Keyword to search in log value (case-insensitive)
            start_time: Optional start of time window
            end_time: Optional end of time window
            cmdb_id: Optional filter for specific service/component

        Returns:
            DataFrame with log entries matching the search
        """
        logger.info(f"Searching logs for keyword: '{keyword}'")

        csv_path = self.get_csv_path()

        if not csv_path.exists():
            logger.warning(f"Log CSV not found: {csv_path}")
            return pd.DataFrame(columns=self.expected_columns + ['datetime'])

        try:
            # Read CSV
            df = pd.read_csv(csv_path)

            # Add datetime column
            if 'timestamp' in df.columns:
                df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')

            # Filter by time window if provided
            if start_time and end_time:
                mask = (df['datetime'] >= start_time) & (df['datetime'] <= end_time)
                df = df[mask]

            # Filter by cmdb_id if provided
            if cmdb_id:
                df = df[df['cmdb_id'] == cmdb_id]

            # Search by keyword (case-insensitive)
            if 'value' in df.columns:
                df = df[df['value'].str.contains(keyword, case=False, na=False)]

            logger.info(f"Found {len(df)} log entries matching '{keyword}'")
            return df

        except Exception as e:
            logger.error(f"Error searching logs: {e}", exc_info=True)
            return pd.DataFrame(columns=self.expected_columns + ['datetime'])

    def get_log_names(self) -> list[str]:
        """
        Get list of all unique log names in the log data.

        Returns:
            List of unique log names
        """
        csv_path = self.get_csv_path()

        if not csv_path.exists():
            logger.warning(f"Log CSV not found: {csv_path}")
            return []

        try:
            # Read log_name column only
            df = pd.read_csv(csv_path, usecols=['log_name'])
            log_names = df['log_name'].unique().tolist()
            logger.info(f"Found {len(log_names)} unique log names")
            return log_names
        except Exception as e:
            logger.error(f"Error reading log names: {e}")
            return []
